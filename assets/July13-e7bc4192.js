import{g as h,l as m,c as d,a as e,b as t,u as s,w as n,r as a,o as f,d as i}from"./index-06db514f.js";import{_ as p}from"./Week1-fe16bf27.js";import{_ as g}from"./Week2-d4919b68.js";import{_ as b}from"./Week3-66c2c813.js";import{_ as $}from"./Week4-72a2f3d0.js";const y="/Presentation/assets/mscm-a11b3c0d.png",v="/Presentation/assets/phase3-1a1ac70a.jpg",x={class:"w-screen h-screen"},w={class:"reveal"},I={class:"slides"},L=e("h2",null,"Object counting",-1),M=e("p",null," The regression-based methods, one of obeject counting methods, is learning to count by regressing a density map, and the predicted count equals the integration of the density map. ",-1),V=e("p",null," Multi-scale counting model(MSCM) is designed based on regression-based method, used to predict the number of each symbol class. It consists of multi-scale feature extraction, channel attention and sum-pooling operator. ",-1),S=e("h2",null,"Multi-scale counting module(MSCM)",-1),C=e("ul",null,[e("li",null," Here, MSCM utilize two parallel convolution branches to extract multi-scale features. Following the convolution layer, the channel attention is adopted to enhance the feature information further. "),e("br"),e("li",null," Following the $1*1$ convolution, it utilize a sigmoid function to yield the value in a range of (0,1) to generate counting map $M \\in \\mathbb{R^{H\\times W}}$. "),e("br"),e("li",null,[i(" In this sense, each $M_i$ is actually a pseudo density map. Then utilize sum-pooling to obtain counting vector $V_i=\\sum_{p=1}^H \\sum_{q=1}^W M_{i,pq}$. "),e("br"),i(" Here, $V_i \\in \\mathbb{R^{1\\times 1}}$ is the predicted count of the i-th class symbol. It is noteworthy that the feature maps of different branches contain different scale information and are highly complementary. ")])],-1),H=e("h2",null,"Loss function",-1),j=e("br",null,null,-1),T=e("br",null,null,-1),N=e("h2",null,"Next Schedule",-1),P=e("h2",null,"Experiment",-1),F={__name:"July13",setup(R){const r=Object.values(Object.assign({"../assets/images/July13/mscm.png":y})),u=h(r);return m(),(k,q)=>{const _=a("Cover"),l=a("HSection"),c=a("Image"),o=a("VSection");return f(),d("div",x,[e("div",w,[e("div",I,[t(_,{date:"July 13"}),e("section",null,[t(l,{text:"I. Recap"}),t(s(p)),t(s(g)),t(s(b)),t(s($))]),e("section",null,[t(l,{text:"II. Auxiliary Loss"}),t(o,null,{default:n(()=>[L,M,V,t(c,{src:s(u).mscm,class:"w-full"},null,8,["src"])]),_:1}),t(o,null,{default:n(()=>[S,C]),_:1}),t(o,null,{default:n(()=>[H,i(" The auxiliary loss function consists of two parts: $L=L_{cls}+L_{counting}$ "),j,i(" $L=L_{cls}$ is a common-used cross entropy classification loss of the predicted probability with respect to its ground-truth. "),T,i(" Denoting the counting ground truth of each symbol class as $\\hat V$, $L_{couting}$ is a smooth $L1$ regression loss defined as follows: $L_{couting} = smooth_{L1}(V, \\hat V)$. ")]),_:1})]),e("section",null,[t(l,{text:"III. Experiment"}),t(o,null,{default:n(()=>[N,t(c,{src:s(v),class:"w-full"},null,8,["src"])]),_:1}),t(o,null,{default:n(()=>[P]),_:1})]),t(l,{text:"Thank you!"})])])])}}};export{F as default};
