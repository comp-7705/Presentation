import{g as c,l as d,c as _,a as e,b as l,u as t,w as s,o as h,m as i}from"./index-7af61a6a.js";import{C as p,V as a,H as r}from"./VSection-f793e86d.js";import{I as o}from"./Image-3b5682a5.js";const g="/Presentation/assets/Exp1n2-Acc-e3baab9a.png",m="/Presentation/assets/Exp1n2-Loss-a1b76c9b.png",b="/Presentation/assets/Exp3n4-Acc-485b442a.png",f="/Presentation/assets/Exp3n4-Loss-2b1edf88.png",v="/Presentation/assets/attention-qkv-700667af.svg",y="/Presentation/assets/densenet-b2364abf.png",w="/Presentation/assets/densenet161-b57dc275.png",x="/Presentation/assets/fpn-228baa35.jpg",$="/Presentation/assets/gru-7c6e684f.svg",R="/Presentation/assets/hmer-b09a4318.jpg",k="/Presentation/assets/pspnet-3270c487.jpg",E="/Presentation/assets/resnet-block-f6e8bcbc.svg",T="/Presentation/assets/resnet152-e40ec634.png",P="/Presentation/assets/schedule-be35df1a.jpg",N={class:"w-screen h-screen"},I={class:"reveal"},S={class:"slides"},A=e("h2",null,"Recap of 1st Week",-1),C=e("ul",null,[e("li",null,[i(" Handwritten Mathematical Expression Recognition (HMER): "),e("br"),i(" Images or Trajectories â‡’ Markup Languages like LaTeX or MathML ")]),e("li",null," Sigificance in Education and Research, in Industry and Business "),e("li",null," CROHME dataset and State-of-the-art models (BTTR, SAN, CoMER) ")],-1),J=e("h2",null,"Recap of 2nd Week: Failure Case Analysis",-1),M=e("ul",null,[e("li",null,[i(" Label Length Distribution: "),e("br"),i(" SAN,BTTR and CoMER are particularly susceptible to failures when the label length is either "),e("span",{class:"text-red-600 font-bold"},"4, 14, or 16"),i(". ")]),e("li",null,[i(" Symbol Recognition Failures: "),e("br"),e("aside",{class:"note"}," All models incorrectly recognized the uppercase $X$ as a lowercase $x$, likely due to ambiguity in the image itself, all models mistook the symbol $\\exists$ for the number $7$, since $\\exists$ is one of the five least frequently occurring tokens. ")]),e("li",null,[i(" Structure Recognition Failures: "),e("br"),e("aside",{class:"note"}," The models BTTR and SAN failed to detect the superscript of $y^{b+1}$. Similarly, BTTR could not include $(n-1)$ within the square root while SAN produced a duplicated value of $>0$. On the other hand, CoMER demonstrated successful recognition of these two examples. ")])],-1),L=e("h2",null,"ResNet",-1),H=e("ul",null,[e("li",null,[i(" When deeper networks are able to start converging, a degradation problem has been exposed: "),e("br"),i(" With the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. ")]),e("li",null," The degradation problem is addressed by introducing a deep residual learning framework. Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual mapping. It is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. ")],-1),j=e("h2",null,"ResNet",-1),z=e("h2",null,"DenseNet",-1),D=e("ul",null,[e("li",null," Problem to solve: Just as the same as ResNet "),e("li",null," To ensure maximum information flow between layers, All layers are connected with other layers (but ensure that feature maps of the same size are present). In order to preserve the characteristics of forward propagation, each layer takes the previous layer as input and passes its own feature map to the subsequent layers. ")],-1),B=e("h2",null,"DenseNet",-1),U=e("h2",null,"Feature Pyramid Network (FPN)",-1),q=e("ul",null,[e("li",null," (a) Scale the images to different scales, and then use the model to predict the images of each scale in turn. "),e("li",null," (b) The final feature map is obtained by performing convolution and pooling operations on the image, and then predictions are made on the final feature map. "),e("li",null," (c) SSD forms a feature pyramid by calculating feature maps from different layers in the convolutional network. "),e("li",null," (d) The features on different feature maps are fused and predictions are made on the fused feature maps. ")],-1),W=e("h2",null,"Pyramid Scene Parsing Network (PSPNet)",-1),O=e("br",null,null,-1),V=e("ul",null,[e("li",null," Mismatched Relationship "),e("li",null," Confusion Categories "),e("li",null," Inconspicuous Classes ")],-1),F=e("br",null,null,-1),G=e("h2",null,"Gated Recurrent Unit (GRU)",-1),X=e("ul",null,[e("li",null,[e("aside",{class:"note"}," Reset Gate $R_t$: $R_t=\\sigma(W_rx_t+U_rH_{t-1}+b_r)$ ")]),e("li",null,[e("aside",{class:"note"}," Update Gate $Z_t$: $Z_t=\\sigma(W_zx_t+U_zH_{t-1}+b_z)$ ")]),e("li",null,[e("aside",{class:"note"},[i(" $\\widetilde{H_t}=\\tanh(W_cx_t,U(r_t\\cdot H_{t-1}))$ "),e("br"),i(" $H_t=z_th_{t-1}+(1-z_t)\\cdot \\widetilde{H_t}$ ")])])],-1),Z=e("h2",null,"Attention Mechanism",-1),K=e("aside",{class:"note"}," The attention mechanism computes a linear combination over values $\\bf{v_i}$ via attention pooling, where weights are derived according to the compatibility between a query $\\bf{q}$ and keys $\\bf{k_i}$ . ",-1),Q=e("h2",null,"Comparison Tables",-1),Y=e("p",null,"Training setting:",-1),ee=e("table",{class:"table-auto border-y-2 border-black"},[e("thead",null,[e("tr",null,[e("th",null,"No. Exp"),e("th",null,"Encoder"),e("th",null,"Decoder"),e("th",null,"Epoch"),e("th",null,"Learning Rate"),e("th",null,"Batch Size")])]),e("tbody",null,[e("tr",null,[e("td",null,"1,2"),e("td",null,"ResNet-152"),e("td",null,"GRU+Attention"),e("td",null,"50"),e("td",null,"1e-4"),e("td",null,"8")]),e("tr",null,[e("td",null,"3,4"),e("td",null,"DenseNet-161"),e("td",null,"GRU+Attention"),e("td",null,"100"),e("td",null,"1e-4"),e("td",null,"8")])])],-1),te=e("p",null,"Loss:",-1),le=e("table",{class:"table-auto border-y-2 border-black"},[e("thead",null,[e("tr",null,[e("th",null,"No. Exp"),e("th",null,"Training"),e("th",null,"2014"),e("th",null,"2016"),e("th",null,"2019"),e("th",null,"Validation (Total)")])]),e("tbody",null,[e("tr",null,[e("td",null,"1,2"),e("td",null,"2.565"),e("td",null,"3.063"),e("td",null,"3.106"),e("td",null,"2.974"),e("td",null,"3.048")]),e("tr",null,[e("td",null,"3,4"),e("td",null,"2.553"),e("td",null,"2.955"),e("td",null,"2.948"),e("td",null,"2.854"),e("td",null,"2.919")])])],-1),ne=e("p",null,"Accuracy (%) on CROHME dataset:",-1),se=e("table",{class:"table-auto border-y-2 border-black"},[e("thead",null,[e("tr",null,[e("th",null,"No. Exp"),e("th",null,"Training"),e("th",null,"2014"),e("th",null,"2016"),e("th",null,"2019"),e("th",null,"Validation (Total)")])]),e("tbody",null,[e("tr",null,[e("td",null,"1,2"),e("td",null,"95.90"),e("td",null,"25.92"),e("td",null,"24.32"),e("td",null,"27.53"),e("td",null,"25.92")]),e("tr",null,[e("td",null,"3,4"),e("td",null,"99.22"),e("td",null,"33.84"),e("td",null,"33.18"),e("td",null,"32.36"),e("td",null,"33.13")])])],-1),ae=e("h2",null,"Training Curves",-1),oe={class:"grid grid-cols-2"},ie=e("h2",null,"Training Curves",-1),re={class:"grid grid-cols-2"},ue=e("h2",null,"Next Schedule",-1),ce={class:"grid grid-cols-2"},de=e("div",null,[e("ul",null,[e("li",null,"Phase I: Data Collection & Pre-processing")]),e("br"),e("br"),e("ul",null,[e("li",null,"Phase II: Model Development & Evaluation")]),e("br"),e("br"),e("ul",null,[e("li",null,"Phase III: Continual Optimization and Model Deployment")])],-1),fe={__name:"June29",setup(_e){const u=Object.values(Object.assign({"../assets/images/June29/Exp1n2-Acc.png":g,"../assets/images/June29/Exp1n2-Loss.png":m,"../assets/images/June29/Exp3n4-Acc.png":b,"../assets/images/June29/Exp3n4-Loss.png":f,"../assets/images/June29/attention-qkv.svg":v,"../assets/images/June29/densenet.png":y,"../assets/images/June29/densenet161.png":w,"../assets/images/June29/fpn.jpg":x,"../assets/images/June29/gru.svg":$,"../assets/images/June29/hmer.jpg":R,"../assets/images/June29/pspnet.jpg":k,"../assets/images/June29/resnet-block.svg":E,"../assets/images/June29/resnet152.png":T,"../assets/images/June29/schedule.jpg":P})),n=c(u);return d(),(he,pe)=>(h(),_("div",N,[e("div",I,[e("div",S,[l(t(p),{date:"June 29"}),l(t(a),null,{default:s(()=>[A,C,l(t(o),{src:t(n).hmer,class:"w-1/3"},null,8,["src"])]),_:1}),l(t(a),null,{default:s(()=>[J,M]),_:1}),e("section",null,[l(t(r),{text:"I. Encoder From Scratch"}),l(t(a),null,{default:s(()=>[L,H,l(t(o),{src:t(n)["resnet-block"],class:"w-2/5"},null,8,["src"])]),_:1}),l(t(a),null,{default:s(()=>[j,i(" We use ResNet152 practically in our model as following. "),l(t(o),{src:t(n).resnet152},null,8,["src"])]),_:1}),l(t(a),null,{default:s(()=>[z,D,l(t(o),{src:t(n).densenet,class:"w-2/5"},null,8,["src"])]),_:1}),l(t(a),null,{default:s(()=>[B,l(t(o),{src:t(n).densenet161,class:"w-2/5"},null,8,["src"])]),_:1}),l(t(a),null,{default:s(()=>[U,q,l(t(o),{src:t(n).fpn,class:"w-1/3"},null,8,["src"])]),_:1}),l(t(a),null,{default:s(()=>[W,i(" In complex-scene parsing,there are 3 common issues:"),O,V,F,i(" On top of the map, we use the pyramid pooling module shown in (c) to gather context information. Using our 4-level pyramid, the pooling kernels cover the whole, half of, and small portions of the image. They are fused as the global prior. Then we concatenate the prior with the original feature map in the final part of (c). It is followed by a convolution layer to generate the final prediction map in (d). "),l(t(o),{src:t(n).pspnet},null,8,["src"])]),_:1})]),e("section",null,[l(t(r),{text:"II. Decoder from Scratch"}),l(t(a),null,{default:s(()=>[G,X,l(t(o),{src:t(n).gru,class:"w-2/1"},null,8,["src"])]),_:1}),l(t(a),null,{default:s(()=>[Z,K,l(t(o),{src:t(n)["attention-qkv"]},null,8,["src"])]),_:1})]),e("section",null,[l(t(r),{text:"III. Experiment"}),l(t(a),null,{default:s(()=>[Q,Y,ee,te,le,ne,se]),_:1}),l(t(a),null,{default:s(()=>[ae,e("div",oe,[l(t(o),{src:t(n)["Exp1n2-Acc"],class:"w-1/1"},null,8,["src"]),l(t(o),{src:t(n)["Exp3n4-Acc"],class:"w-1/1"},null,8,["src"])])]),_:1}),l(t(a),null,{default:s(()=>[ie,e("div",re,[l(t(o),{src:t(n)["Exp1n2-Loss"],class:"w-1/1"},null,8,["src"]),l(t(o),{src:t(n)["Exp3n4-Loss"],class:"w-1/1"},null,8,["src"])])]),_:1})]),l(t(a),null,{default:s(()=>[ue,e("div",ce,[de,l(t(o),{src:t(n).schedule,class:"w-4/5"},null,8,["src"])])]),_:1}),l(t(r),{text:"Thank you!"})])])]))}};export{fe as default};
